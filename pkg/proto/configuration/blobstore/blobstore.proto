syntax = "proto3";

package buildbarn.configuration.blobstore;

import "google/rpc/status.proto";
import "google/protobuf/duration.proto";
import "pkg/proto/configuration/grpc/grpc.proto";
import "pkg/proto/configuration/tls/tls.proto";

option go_package = "github.com/buildbarn/bb-storage/pkg/proto/configuration/blobstore";

message AzureBlobAccessConfiguration {
  // Azure storage account name.
  string account_name = 1;

  // The key associated to the account.
  string account_key = 2;

  // The name of the storage container to use.
  string container_name = 3;
}

// Storage configuration for Bazel Buildbarn.
message BlobstoreConfiguration {
  // Storage configuration for the Content Addressable Storage (CAS).
  BlobAccessConfiguration content_addressable_storage = 1;

  // Storage configuration for the Action Cache (AC).
  BlobAccessConfiguration action_cache = 2;
}

message BlobAccessConfiguration {
  oneof backend {
    // Read objects from/write objects to a Redis server.
    RedisBlobAccessConfiguration redis = 2;

    // Read objects from/write objects to a Bazel remote build cache server.
    RemoteBlobAccessConfiguration remote = 3;

    // Split up objects across two storage backends by digest size.
    SizeDistinguishingBlobAccessConfiguration size_distinguishing = 5;

    // Read objects from/write objects to a circular file on disk.
    CircularBlobAccessConfiguration circular = 6;

    // Read objects from/write objects to a GRPC service that
    // implements the remote execution protocol.
    buildbarn.configuration.grpc.GRPCClientConfiguration grpc = 7;

    // Always fail with a fixed error response.
    google.rpc.Status error = 8;

    // Fan out requests across multiple storage backends to spread
    // out load.
    ShardingBlobAccessConfiguration sharding = 9;

    // Read objects from/write objects to various cloud-based backends.
    CloudBlobAccessConfiguration cloud = 10;

    // Store blobs on the local system.
    //
    // TODO: Right now this backend can only be used to store data in
    // memory. We should work towards letting this backend replace
    // circular by supporting on-disk storage.
    LocalBlobAccessConfiguration local = 15;
  }
}

message CircularBlobAccessConfiguration {
  // Directory where the files created by the circular file storage
  // backend are located.
  string directory = 1;

  // Maximum size of the hash table containing data offsets.
  uint64 offset_file_size_bytes = 2;

  // Maximum size of the circular file containing data.
  uint64 data_file_size_bytes = 3;

  // Number of offset entries to cache in memory.
  uint32 offset_cache_size = 4;

  // Instances for which to store entries. For the Content Addressable
  // Storage, this field may be omitted, as data for all instances is
  // stored together. For the Action Cache, this field is required, as
  // every instance needs its own offset file and cache.
  repeated string instances = 5;

  // Amount of space to allocate in the data file at once. Setting
  // this value too low may cause an excessive number of writes to the
  // state file. Setting this value too high may cause excessive
  // amounts of old data to be invalidated upon process restart.
  uint64 data_allocation_chunk_size_bytes = 6;
}

message CloudBlobAccessConfiguration {
  // Prefix for keys, e.g. 'bazel_cas/'.
  string key_prefix = 1;

  // Backend configuration
  oneof config {
    // Url of the bucket to use, currently supports the following schemes:
    // mem, file, s3, gs, azblob.
    string url = 2;
    AzureBlobAccessConfiguration azure = 3;
    GCSBlobAccessConfiguration gcs = 4;
    S3BlobAccessConfiguration s3 = 5;
  }
}

message GCSBlobAccessConfiguration {
  // Name of the bucket to use.
  string bucket = 1;

  // The JWT credentials to authenticate against GCP.
  string credentials = 2;
}

message ClusteredRedisBlobAccessConfiguration {
  // Endpoint addresses of the Redis servers.
  repeated string endpoints = 1;

  // Retry configuration, defaults to 16 if not set
  uint32 maximum_retries = 2;

  // After each attempt, the delay will be randomly selected from values
  // between: 0 and min((2^attempt * minimum_retry_backoff),
  // maximum_retry_backoff)
  google.protobuf.Duration minimum_retry_backoff = 3;
  google.protobuf.Duration maximum_retry_backoff = 4;
}

message SingleRedisBlobAccessConfiguration {
  // Endpoint address of the Redis server (e.g., "localhost:6379").
  string endpoint = 1;

  // Numerical ID of the database.
  int32 db = 2;
}

message RedisBlobAccessConfiguration {
  oneof mode {
    // Redis is configured in clustered mode.
    ClusteredRedisBlobAccessConfiguration clustered = 1;

    // Redis is configured as a single server.
    SingleRedisBlobAccessConfiguration single = 2;
  }

  // TLS configuration for the Redis connection. TLS will not be enabled
  // when not set.
  buildbarn.configuration.tls.TLSClientConfiguration tls = 4;

  // How long to keep a cache key in Redis, specified as a duration.
  // When unset, this means keys do not expire and and rely on Redis
  // eviction policy to efficiently remove keys when storage gets full.
  // A reasonable number for this would allow keys to live long enough
  // objects to be found in the CAS once the client uploads them.
  google.protobuf.Duration key_ttl = 7;

  // The minimum number of replicas to successfully replicate put calls to
  // before considering it successful.
  // If unset, no guarantee is made on the number of replicas that contain
  // the contents of a Redis write and master failures can lose data.
  int64 replication_count = 8;

  // The maximum tolerated replication delay expressed in seconds.
  // If unset, Redis write calls return immediately and no attempt is made to
  // ensure that replication succeeds. This can result in data loss when
  // the master is lost.
  google.protobuf.Duration replication_timeout = 9;
}

message RemoteBlobAccessConfiguration {
  // URL of the remote build cache (e.g., "http://localhost:8080/").
  string address = 1;
}

message S3BlobAccessConfiguration {
  // URL of the S3 bucket (e.g., "http://localhost:9000" when using Minio).
  string endpoint = 1;

  // AWS Access Key ID. If unspecified, AWS will search the default credential
  // provider chain.
  string access_key_id = 2;

  // AWS Secret Access Key.
  string secret_access_key = 3;

  // AWS region (e.g., "eu-west-1").
  string region = 4;

  // Whether SSL should be disabled.
  bool disable_ssl = 5;

  // Name of the S3 bucket.
  string bucket = 6;
}

message ShardingBlobAccessConfiguration {
  message Shard {
    // Storage backend that is used by this shard. Omitting this
    // causes the implementation to assume this shard is drained.
    // Requests to this shard will be spread out across the other
    // shards.
    BlobAccessConfiguration backend = 1;

    // Non-zero ratio of how many keys are allocated to this shard.
    // When all shards have equal specifications (i.e., capacity and
    // bandwidth), every shard may have a weight of one.
    //
    // For the backend selection algorithm to run quickly, it is not
    // not advised to let the total weight of drained backends
    // strongly exceed the total weight of undrained ones.
    uint32 weight = 2;
  }

  // Initialization for the hashing algorithm used to partition the
  // key space. This should be a random 64-bit value that is unique to
  // this deployment. Failure to do so may result in poor distribution
  // in case sharding is nested.
  //
  // Changing this value will in effect cause a full repartitioning of
  // the data.
  uint64 hash_initialization = 1;

  // Shards to which requests are routed. To reduce the need for full
  // repartitioning of the data when growing a cluster, it's possible
  // to terminate this list with a drained backend that increases the
  // total weight up to a given number. Newly added backends may
  // allocate their weight from this backend, thereby causing most of
  // the keyspace to still be routed to its original backend.
  repeated Shard shards = 2;
}

message SizeDistinguishingBlobAccessConfiguration {
  // Backend to which to send requests for small blobs (e.g., Redis).
  BlobAccessConfiguration small = 1;

  // Backend to which to send requests for large blobs (e.g., S3).
  BlobAccessConfiguration large = 2;

  // Maximum size of blobs read from/written to the backend for small blobs.
  int64 cutoff_size_bytes = 3;
}

message LocalBlobAccessConfiguration {
  // The digest-location map is a hash table that is used by this
  // storage backend to resolve digests to locations where data is
  // stored. Because entries are small (64 bytes in size), it is
  // recommended to make this map relatively large to reduce collisions.
  //
  // Recommended value: between 2 and 10 times the expected number of
  // objects stored.
  int64 digest_location_map_size = 1;

  // The number of indices a Get() call on the digest-location map may
  // attempt to access. The lower the utilization rate of the
  // digest-location map, the lower this value may be set. For example,
  // if the size of the digest-location map is set in such a way that it
  // is only utilized by 10% (factor 0.1), setting this field to 16
  // means there is only a 0.1^8 chance that inserting an entry
  // prematurely displaces another object from storage.
  //
  // Recommended value: 8
  uint32 digest_location_map_maximum_get_attempts = 2;

  // The number of mutations that a Put() on the digest-location map may
  // perform. Because the digest-location map uses a scheme similar to
  // cuckoo hashing, insertions may cause other entries to be displaced.
  // Those entries may then cause even more entries to be displaced.
  // Because of that, it is recommended to set this field to a small
  // multiple of the maximum Get() attempts.
  //
  // Recommended value: 32
  int64 digest_location_map_maximum_put_attempts = 3;

  // Data is stored in a list of blocks. The total number of blocks
  // constant over time, with small fluctuations to deal with lingering
  // requests when removing a block.
  //
  // Recommended value: (total space available) /
  //                    (old_blocks + current_blocks + new_blocks)
  int64 block_size_bytes = 4;

  // The number of blocks, where attempting to access any data stored
  // within will cause it to be refreshed (i.e., copied into new
  // blocks).
  //
  // Setting the number of old blocks too low may cause builds to fail,
  // due to data disappearing prematurely. Setting the number of old
  // blocks too high may cause an excessive amount of duplication in the
  // data set. For example, if old_blocks == current_blocks + new_blocks,
  // there may be a redundancy in the data set up to a factor of two.
  //
  // Recommended value: 8
  int32 old_blocks = 5;

  // The number of blocks, where attempting to access data stored within
  // will not cause data to be refreshed immediately. The containing
  // block will first need to become old for data to be eligible for
  // refreshes.
  //
  // Recommended value: 24
  int32 current_blocks = 6;

  // The number of blocks where new data needs to be written. It is
  // valid to set this to just 1. Setting it to a slightly higher value
  // has the advantage that frequently used objects will over time get
  // smeared out across the data set. This spreads out the cost
  // refreshing data from old to new blocks.
  //
  // Because the probability of storing objects in new blocks has an
  // inverse exponential distribution, it is not recommended to set this
  // to any value higher than 4. Whereas the first new block will at
  // times be somewhere between 50% and 100% full, the fourth new block
  // will only be between 6.25% and 12.5% full, which is wasteful.
  //
  // Recommended value: 3
  int32 new_blocks = 7;

  // Instances for which to store objects. For the Content Addressable
  // Storage, this field may be omitted, as data for all instances is
  // stored together. For the Action Cache, this field is required, as
  // every instance needs its own digest-location map.
  repeated string instances = 8;
}
